{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabefb97-80ca-4bc3-850e-d0dff3d533f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import datetime\n",
    "import torch\n",
    "import os\n",
    "import json\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.docstore.document import Document\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6b69c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]=\"YOUR_HUGGINGFACE_API_TOKEN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e61ff90-84c3-4044-9ccf-5aa757c07538",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../Data/iot_readable_last_en.txt\"\n",
    "encoder_name = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    "encoder_model_lib = \"transformer\"\n",
    "decoder_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "vectorstore_path = \"../Data/iot_data_faiss_en_new\"\n",
    "local_decoder_path = \"../Models/model-phi-3-4k-mini-instruct\"\n",
    "local_tokenizer_path = \"../Models/tokenizer-phi-3-4k-mini-instruct\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "011ef76e-4391-41aa-a5da-efa68f2b4e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vectorstore():\n",
    "    if not os.path.exists(vectorstore_path):\n",
    "        # LOAD -> SPLIT -> EMBED -> STORE -> RETRIEVE\n",
    "        loader = TextLoader(data_path, encoding=\"utf-8\")\n",
    "        text_file = loader.load()\n",
    "\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            separators=[\"\\n\"],\n",
    "            chunk_size = 1,\n",
    "            chunk_overlap = 0\n",
    "        )\n",
    "\n",
    "        # Metadata Çıkarma İşlemi\n",
    "        print(\"Metadata Çıkarılıyor...\")\n",
    "        metadata = []\n",
    "        for line in text_file[0].page_content.split(\"\\n\"):\n",
    "            parts = [p.strip() for p in line.split(\",\")]\n",
    "\n",
    "            if len(parts) <= 1:\n",
    "                break\n",
    "            \n",
    "            date = parts[0].split(\":\")[1].strip()\n",
    "            hour = parts[1].split(\":\", 1)[1].strip()\n",
    "            room = parts[2].split(\":\")[1].strip()\n",
    "            sensor = parts[3].split(\":\")[1].strip()\n",
    "\n",
    "            dtime = datetime.datetime.strptime(f\"{date} {hour}\", \"%Y-%m-%d %H:%M\")\n",
    "            timestamp = dtime.timestamp()\n",
    "\n",
    "            metadata.append({\"Date\": date, \"Hour\": hour, \"Room\": room, \"Sensor\": sensor})\n",
    "        print(\"Metadata Hazır!\\n\")\n",
    "\n",
    "        print(\"Metinler Ayrıştırılıyor...\")\n",
    "        splitted_texts = text_splitter.split_text(text_file[0].page_content)\n",
    "        print(f\"Metinden şu kadar satır çıkarıldı: {len(splitted_texts)}\")\n",
    "        splitted_texts = splitted_texts[:-1]\n",
    "        print(f\"Veri olarak seçilen satır sayısı: {len(splitted_texts)}\")\n",
    "        print(f\"Metadata listesi uzunluğu: {len(metadata)}\\n\")\n",
    "\n",
    "        docs = [Document(page_content=text, metadata=meta) for text, meta in zip(splitted_texts, metadata)]\n",
    "\n",
    "        print(f\"Embedding Modeli Yükleniyor... Model Kütüphanesi: {encoder_model_lib}\")\n",
    "        if encoder_model_lib == \"sentence-transformer\":\n",
    "            pass\n",
    "        elif encoder_model_lib == \"transformer\":\n",
    "            embeddings = HuggingFaceEmbeddings(\n",
    "                model_name=encoder_name,\n",
    "                model_kwargs={\"device\": \"cpu\"},\n",
    "                encode_kwargs={\"normalize_embeddings\": False}\n",
    "            )\n",
    "        elif encoder_model_lib == \"google-genai\":\n",
    "            pass\n",
    "        print(\"Embedding Modeli Hazır!\\n\")\n",
    "\n",
    "        print(\"Metadatalı Vektör Veritabanı Başlatılıyor...\")\n",
    "        vectorstore = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "        print(\"Metadatalı Vektör Veritabanı Hazır!\")\n",
    "    \n",
    "    else:\n",
    "        print(f\"Metadatalı Vektör Veritabanı {vectorstore_path} Klasöründen Yükleniyor...\")\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=encoder_name,\n",
    "            model_kwargs={\"device\": \"cpu\"},\n",
    "            encode_kwargs={\"normalize_embeddings\": False}\n",
    "        )\n",
    "        \n",
    "        vectorstore = FAISS.load_local(vectorstore_path, embeddings=embeddings, allow_dangerous_deserialization=True)\n",
    "        print(\"Metadatalı Vektör Veritabanı Başarıyla Yüklendi!\")\n",
    "\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4ba3dc9b-ca84-43df-9d81-8c2df79936ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_model(user_query : str):\n",
    "    route_msg_template = f\"\"\"\n",
    "    You are an expert at routing a user's question to the correct data source.\n",
    "\n",
    "    Based on the user's question, decide whether to route them to the 'vectorstore' or 'general_knowledge'.\n",
    "    Make sure you only return decision that you made.\n",
    "\n",
    "    'vectorstore': This is for questions that ask for specific, historical, or current data points from the smart home sensors. \n",
    "    Examples:\n",
    "    - What is the current temperature in the living room?\n",
    "    - What was the humidity in the bedroom yesterday at 5 PM?\n",
    "    - Show me the sensor readings for the last hour.\n",
    "\n",
    "    'general_knowledge': This is for questions that ask for advice, explanations, how-to guides, or general conversation that does not require specific sensor data.\n",
    "    Examples:\n",
    "    - How can I reduce the temperature in a room?\n",
    "    - Why is humidity important?\n",
    "    - Hello, how are you today?\n",
    "\n",
    "    User Question:\n",
    "    {user_query}\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\" : \"user\",\n",
    "            \"content\" : route_msg_template\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3312b632-9f87-4038-aca0-011ac25e43b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_documents(structured_query, vectorstore, metadatas):\n",
    "    retriever = vectorstore.as_retriever(search_type=\"mmr\", search_kwargs={\"k\": 10, \"fetch_k\": 300, \"filter\": {\"Hour\": metadatas[\"Hour\"]}})\n",
    "    search_result = retriever.invoke(structured_query)\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a423c993-b68c-415d-8bec-dd42ead1bdd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_decoder_tokenizer():\n",
    "    if os.path.exists(local_tokenizer_path):\n",
    "        print(f\"Decoder Tokenizeri {local_tokenizer_path} Klasöründen Yükleniyor...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(local_tokenizer_path)\n",
    "        print(f\"Decoder Tokenizeri {local_tokenizer_path} Klasöründen Başarıyla Yüklendi!\\n\")\n",
    "    else:\n",
    "        print(\"Local Tokenizer Bulunamadı Online Olarak İndiriliyor...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(decoder_name)\n",
    "        print(\"Tokenizer Başarıyla İndirildi!\\n\")\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96b84c64-a021-433f-aad5-f8231aa32634",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_decoder():\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"Sistemdeki aktif GPU sayısı: {torch.cuda.device_count()}\")\n",
    "        print(f\"Sistemdeki Ana GPU'nun Adı: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "        print(\"BNB Config Hazırlanıyor...\")\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit = True,\n",
    "            bnb_4bit_quant_type = \"nf4\",\n",
    "            bnb_4bit_compute_dtype = torch.bfloat16\n",
    "        )\n",
    "        print(\"BNB Config Hazır!\\n\")\n",
    "    else:\n",
    "        print(\"Sistemde çalışan GPU Bulunamıyor.\")\n",
    "        print(\"Model CPU üzerinde koşturulacak.\\n\")\n",
    "        quantization_config = None\n",
    "\n",
    "    if os.path.exists(local_decoder_path):\n",
    "        if quantization_config:\n",
    "            print(f\"Decoder Modeli {local_decoder_path} Klasöründen Quantize Edilerek Yükleniyor...\")\n",
    "            print(f\"Modelin Yükleneceği GPU: {torch.cuda.get_device_name(0)}\")  \n",
    "            model = AutoModelForCausalLM.from_pretrained(local_decoder_path, quantization_config=quantization_config, device_map=\"auto\")\n",
    "            print(f\"Decoder Modeli {local_decoder_path} Klasöründen Başarıyla Yüklendi!\")\n",
    "        else:\n",
    "            print(f\"Decoder Modeli {local_decoder_path} Klasöründen Tam Olarak Yükleniyor...\")\n",
    "            print(\"Decoder Modeli CPU Üzerinden Koşturulacak.\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(local_decoder_path, device_map=\"auto\")\n",
    "            print(f\"Decoder Modeli {local_decoder_path} Klasöründen Başarıyla Yüklendi!\")\n",
    "    else:\n",
    "        print(\"Local Model Bulunamadı Online Olarak İndiriliyor...\")\n",
    "        if quantization_config:\n",
    "            print(f\"Decoder modeli {torch.cuda.get_device_name(0)} isimli GPU üzerine yükleniyor...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(decoder_name, quantization_config=quantization_config, device_map=\"auto\")\n",
    "        else:\n",
    "            print(f\"Decoder modeli CPU üzerine yükleniyor...\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(decoder_name, device_map=\"auto\")\n",
    "        print(\"Decoder Modeli Başarıyla Yüklendi!\")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07a8b8ab-bc99-4415-b3c8-ecb9a26a0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_query_prompt(user_query : str):\n",
    "    current_year = datetime.datetime.now().year\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"\n",
    "    Your task is to extract specific fields from the given input sentence and return the result strictly in JSON format.\n",
    "    \n",
    "    Rules:\n",
    "    - Always include these 4 fields in the JSON: \"date\", \"time\", \"room\", \"query\".\n",
    "    - If the sentence has a specific calendar date (e.g., \"16 August 2025\"), extract it in ISO format YYYY-MM-DD if possible.\n",
    "      If the year is not given, return only the given part (e.g., \"16 August\").\n",
    "    - If no explicit date is mentioned, return \"not mentioned\".\n",
    "    - If year is not mentioned use current year, Current Year: {current_year}\n",
    "    - For the \"time\" field:\n",
    "        * If the query has explicit time (e.g., \"9 A.M.\"), convert it to 24-hour format (e.g., \"09:00\").\n",
    "        * If the query uses relative expressions like \"yesterday\", \"today\", \"current\", \"last time\", keep them as they are.\n",
    "        * If no time expression exists, return \"not mentioned\".\n",
    "    - \"room\" should always be extracted from the input (e.g., \"living room\", \"bedroom\").\n",
    "    - \"query\" should capture what is being asked (e.g., \"temperature\", \"humidity\").\n",
    "    \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": \"Understood. I will extract the fields from the input sentence and provide the output in JSON format. Please provide the first input.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Input: What is the current temperature in the living room?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '{\"date\": \"current\", \"time\": \"not mentioned\", \"room\": \"living room\", \"query\": \"temperature\"}'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Input: What was the temperature in the bedroom yesterday?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '{\"date\": \"yesterday\", \"time\": \"not mentioned\", \"room\": \"bedroom\", \"query\": \"temperature\"}'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Input: What was the temperature in the bedroom last time?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '{\"date\": \"last time\", \"time\": \"not mentioned\", \"room\": \"bedroom\", \"query\": \"temperature\"}'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Input: What is the current humidity level in the sitting room?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '{\"date\": \"current\", \"time\": \"not mentioned\", \"room\": \"sitting room\", \"query\": \"humidity\"}'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Input: What was the humidity level in living room yesterday at 5 P.M\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '{\"date\": \"yesterday\", \"time\": \"17:00\", \"room\": \"living room\", \"query\": \"humidity\"}'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Input: What was the temperature in bathroom yesterday\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"asisstant\",\n",
    "            \"content\": '{\"date\": \"yesterday\", \"time\": \"not mentioned\", \"room\": \"bathroom\", \"query\": \"temperature\"}'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Input: What was the temperature in the living room at the 16 August 2025 9 A.M?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '{\"date\": \"2025-08-16\", \"time\": \"09:00\", \"room\": \"living room\", \"query\": \"temperature\"}'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Input: What was the humidity level in bedroom at 20 August\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": '{\"date\": \"2025-08-16, \"time\": \"not mentioned\", \"room\": \"bedroom\", \"query\": \"humidity\"}'\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Input: {user_query}\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29146430-db0d-40ed-b336-21f8b9d91223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query_and_extract_metadata(model_output : str):\n",
    "    \"\"\"\n",
    "    Gelecek stringde 4 alan var, bu alanlar şunlar:\n",
    "    - date\n",
    "    - time\n",
    "    - room\n",
    "    - query\n",
    "\n",
    "    date: ['current', 'yesterday', 'now', 'last time' or date format e.g '2025-08-16']\n",
    "    time: ['not mentioned' or 24 hour format e.g '09:00']\n",
    "    room: ['kitchen', 'bedroom', 'living room', 'bathroom' ...]\n",
    "    query: ['temperature', 'humidity']\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        temp = json.loads(model_output)\n",
    "    except:\n",
    "        print(f\"Şu çıktıyı JSON objesine çevirmeye çalışırken hata oluştu:\\n{model_output}\")\n",
    "\n",
    "    try:\n",
    "        date = temp['date']\n",
    "        time = temp['time']\n",
    "        room = temp['room']\n",
    "        query = temp['query']\n",
    "    except:\n",
    "        print(f\"JSON objesinden valueleri almaya çalışırken hata meydana geldi {temp}\")\n",
    "\n",
    "    current = datetime.datetime.now()\n",
    "\n",
    "    current_date = current.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    current_hour = current.strftime(\"%H:%M\")\n",
    "    current_hour = datetime.datetime.strptime(current_hour, \"%H:%M\")\n",
    "    new_min = (current_hour.minute // 5) * 5\n",
    "    rounded_hour = current_hour.replace(minute=new_min, second=0, microsecond=0)\n",
    "\n",
    "    rounded_hour = rounded_hour.strftime(\"%H:%M\")\n",
    "    \n",
    "    yesterday = current - datetime.timedelta(days=1)\n",
    "    yesterday_date = yesterday.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    if ((date.lower() == \"not mentioned\") or (date.lower() == \"current\") or (date.lower() == \"now\") or (date.lower() == \"last time\")):\n",
    "        new_date = current_date\n",
    "    elif (date.lower() == \"yesterday\"):\n",
    "        new_date = yesterday_date\n",
    "    else:\n",
    "        new_date = date\n",
    "\n",
    "    if (time.lower() == \"not mentioned\"):\n",
    "        new_time = rounded_hour\n",
    "    else:\n",
    "        new_time = time\n",
    "\n",
    "    if ((time.lower() == \"yesterday\")):\n",
    "        content = f\"What was the {query} in {room} on {date} at {new_time}\"\n",
    "    else:\n",
    "        content = f\"What is the {query} in {room} on {date} at {new_time}\"\n",
    "\n",
    "    metadata = {\"Date\": new_date, \"Hour\": new_time, \"Room\": room.capitalize(), \"Sensor\":  query.capitalize()+\" Sensor\" }\n",
    "\n",
    "    return content, metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6a6c2dd0-bee9-4321-9c94-04a6473991ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_answering_prompt(user_query : str, model_decision : str, docs_from_faiss=None, metadata_info=None):\n",
    "    if \"general_knowledge\" in model_decision:\n",
    "        answering_prompt_template = f\"\"\"\n",
    "        You are an smart home assistant that answers user questions clearly and directly using general knowledge. \n",
    "        Keep your responses concise and focused without unnecessary small talk such as \"Would you like to know more?\" or \"Should I also explain this?\". \n",
    "        Avoid giving identical answers every time by slightly varying your wording or adding small creative touches, while still keeping the response accurate and relevant to the user’s question. \n",
    "        You can provide information on the following topics: smart home sensors, general knowledge, or the current measurement values from sensors in specific parts of the house.\n",
    "        You are an assistant that only answers user questions. You only have access to the database. You cannot control or change smart devices, like turning them on or off.\n",
    "        **If you do not know the answer to a question, state that you do not have the information rather than guessing.**\n",
    "        Make sure you just return the answer that you made.\n",
    "        Your goal is to be helpful, informative, and consistent without being repetitive or chatty.\n",
    "\n",
    "        Now answer the user's question with your general knowledge:\n",
    "        User Question: {user_query}\n",
    "        \"\"\"\n",
    "\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\" : \"user\",\n",
    "                \"content\" : answering_prompt_template \n",
    "            }\n",
    "        ]\n",
    "    else:\n",
    "        answering_prompt_template = \"\"\"\n",
    "        You are a helpful and concise smart home assistant. Your primary goal is to answer the user's question based strictly on the context provided under \"[Provided Information]\" and \"[Metadata]\". Follow all rules precisely.\n",
    "\n",
    "        Here are the rules and examples for how you must respond.\n",
    "\n",
    "        ### Rules\n",
    "        1.  Your main goal is to answer the user's question using only the informations within the '[Provided Information]' and '[Metadata]' blocks.\n",
    "        2.  If the information is sufficient to answer the question, synthesize it into a complete, natural, and helpful English sentence. Do not just state the number.\n",
    "        3.  If the '[Provided Information]' block is empty, does not contain relevant details, or is insufficient to answer the question, you must respond with the exact phrase: \"I don't have enough information on this topic, please try again later.\"\n",
    "        4.  After providing the direct answer based on the information, you are allowed to add one short, relevant, and interesting fact in English if you are highly confident about it. This extra fact must start with the prefix \"Info: \". Do not add this if you have no information.\n",
    "\n",
    "        ### Examples\n",
    "\n",
    "        ## Example 1: (Sufficient Information)\n",
    "        [User Question]\n",
    "        What was the temperature in the sitting room yesterday?\n",
    "\n",
    "        [Provided Information]\n",
    "        Date: 2025-08-15, Time: 9:00, Room: Sitting Room, Sensor: Temperature Sensor, Sensor Value: 23.4, Sensor Unit: °C, Status: Low Temperature\n",
    "\n",
    "        [Metadata]\n",
    "        {{\"Date\": \"2025-08-15\", \"Hour\": \"09:00\", \"Room\": \"Sitting room\", \"Sensor\": \"Temperature Sensor\"}}\n",
    "\n",
    "        [Your Answer]\n",
    "        The temperature recorded in the sitting room yesterday was 23.4°C, which was considered low.\n",
    "\n",
    "        ## Example 2: (Insufficient Information)\n",
    "        [User Question]\n",
    "        What is the orbit of Mars?\n",
    "\n",
    "        [Provided Information]\n",
    "        \"No information found this topic.\"\n",
    "\n",
    "        [Metadata]\n",
    "            \n",
    "        [Your Answer]\n",
    "        I don't have enough information on this topic, please try again later.\n",
    "\n",
    "        ## Example 3: (Sufficient Information + Extra Fact)\n",
    "        [User Question]\n",
    "        What is the current temperature in the living room?\n",
    "\n",
    "        [Provided Information]\n",
    "        Date: 2025-08-16, Time: 15:00, Room: Living Room, Sensor: Temperature Sensor, Sensor Value: 25.4, Sensor Unit: °C, Status: Normal\n",
    "\n",
    "        [Metadata]\n",
    "        {{\"Date\": 2025-08-16, \"Hour\": 15:00, \"Room\": Living room, \"Sensor\": \"Temperature Sensor\"}}\n",
    "\n",
    "        [Your Answer]\n",
    "        The current temperature in the living room is 25.4°C, which is a normal value. Info: The ideal room temperature for humans is generally considered to be between 20-22°C.\n",
    "\n",
    "\n",
    "        ## Now, answer the user's actual question based on the provided information and metadata information:\n",
    "\n",
    "        [User Question]\n",
    "        {user_question_placeholder}\n",
    "\n",
    "        [Provided Information]\n",
    "        {context_from_faiss_placeholder}\n",
    "\n",
    "        [Metadata]\n",
    "        {metadata_info_placeholder}\n",
    "\n",
    "        [Your Answer]\n",
    "        \"\"\"\n",
    "\n",
    "        if docs_from_faiss:\n",
    "            context = \"\\n\".join([doc.page_content for doc in docs_from_faiss])\n",
    "        else:\n",
    "            context = \"No document infortmation found this topic.\"\n",
    "        \n",
    "        if not metadata_info:\n",
    "            metadata_info = \"No metadata information found for this topic.\"\n",
    "\n",
    "        answering_prompt_final = answering_prompt_template.format(\n",
    "            user_question_placeholder = user_query,\n",
    "            context_from_faiss_placeholder = context,\n",
    "            metadata_info_placeholder = metadata_info\n",
    "        )\n",
    "\n",
    "        messages = [\n",
    "            {\"role\": \"user\", \"content\": answering_prompt_final}\n",
    "        ]\n",
    "\n",
    "    return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cbde146c-245a-45a6-b0b5-59e628960790",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_invoke(messages : list, tokenizer, model):\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "    answer = model.generate(inputs, max_new_tokens=150, temperature=0.7, eos_token_id=tokenizer.eos_token_id)\n",
    "    result = tokenizer.decode(answer[0][inputs.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66902a15-ab83-4cc6-a0a9-37d13d574941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadatalı Vektör Veritabanı ../Data/iot_data_faiss_en_new Klasöründen Yükleniyor...\n",
      "Metadatalı Vektör Veritabanı Başarıyla Yüklendi!\n",
      "Sistemdeki aktif GPU sayısı: 1\n",
      "Sistemdeki Ana GPU'nun Adı: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "BNB Config Hazırlanıyor...\n",
      "BNB Config Hazır!\n",
      "\n",
      "Decoder Modeli ../Models/model-phi-3-4k-mini-instruct Klasöründen Quantize Edilerek Yükleniyor...\n",
      "Modelin Yükleneceği GPU: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "Decoder Modeli ../Models/model-phi-3-4k-mini-instruct Klasöründen Başarıyla Yüklendi!\n",
      "Decoder Tokenizeri ../Models/tokenizer-phi-3-4k-mini-instruct Klasöründen Yükleniyor...\n",
      "Decoder Tokenizeri ../Models/tokenizer-phi-3-4k-mini-instruct Klasöründen Başarıyla Yüklendi!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vectorstore = initialize_vectorstore()\n",
    "decoder_model = initialize_decoder()\n",
    "decoder_tokenizer = initialize_decoder_tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e99b41aa-7861-404b-9532-d9a2acbd6444",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_1 = \"Hello how are you today?\"\n",
    "\n",
    "route_prompt = route_model(user_query=user_query_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ebd87bf-def5-45db-8314-3963cf86acf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "slm_decision = model_invoke(route_prompt, tokenizer=decoder_tokenizer, model=decoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b89547b3-aa19-4084-84ac-49ee36be5648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"'general_knowledge'\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slm_decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "235a9471-b713-4f2f-9051-523317e3f179",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "if 'general_knowledge' in slm_decision:\n",
    "    answering_prompt = create_answering_prompt(user_query=user_query_1, model_decision=slm_decision)\n",
    "    model_response = model_invoke(answering_prompt, tokenizer=decoder_tokenizer, model=decoder_model)\n",
    "else:\n",
    "    print(\"Model veritabanına gerek duyduğuna karar verdi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06eedca1-ad57-4923-b724-1a2addc21429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I'm an AI, so I don't have feelings, but I'm fully operational and ready to assist you.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b148a9ce-67e7-4a82-9c33-cb4895eaf701",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_2 = \"What can i ask you?\"\n",
    "\n",
    "route_prompt_2 = route_model(user_query=user_query_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "eef19fe8-e23e-43d7-ae1d-31ff04a97a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "slm_decision_2 = model_invoke(route_prompt_2, tokenizer=decoder_tokenizer, model=decoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c8caf545-6063-4d29-adac-04f96fda2676",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'general_knowledge'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "slm_decision_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1d5b3d51-63c4-413a-96af-68b8b35c74d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "if 'general_knowledge' in slm_decision:\n",
    "    answering_prompt_2 = create_answering_prompt(user_query=user_query_2, model_decision=slm_decision_2)\n",
    "    model_response_2 = model_invoke(answering_prompt_2, tokenizer=decoder_tokenizer, model=decoder_model)\n",
    "else:\n",
    "    print(\"Model veritabanına gerek duyduğuna karar verdi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4eeddf6a-9e98-4711-b700-1714a7959d3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You can ask me about the status of smart home sensors, general knowledge topics, or current measurements from various sensors in your house.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_response_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fea0fe6-884d-4d84-b581-6c1877d48f4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
