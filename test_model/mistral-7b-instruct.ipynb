{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2de26cf5-25a9-4d9c-a039-91c843c15f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Berkay Kaya\\Desktop\\BEKO_Staj\\Proje\\ai-assistant\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8955effa-f047-4124-9045-4617324fc168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch CUDA erişilebilir mi: True\n",
      "Ortamdaki GPU Sayısı: 1\n",
      "Ortamdaki Ana GPU Adı: NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch CUDA erişilebilir mi: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Ortamdaki GPU Sayısı: {torch.cuda.device_count()}\")\n",
    "    print(f\"Ortamdaki Ana GPU Adı: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ac60a7-fa34-4c30-83e8-500adcc4041a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "#quantization_config = BitsAndBytesConfig(\n",
    "#    load_in_4bit=True,\n",
    "#    bnb_4bit_quant_type=\"nf4\",\n",
    "#    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc8fba29-92a4-4b10-b0c7-dd1749a3eca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████| 3/3 [00:07<00:00,  2.54s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu and disk.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8388c815-aaf2-4d11-93f0-58f708b52788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving checkpoint shards: 100%|██████████████████████████████████████████████████████████| 6/6 [04:00<00:00, 40.14s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"../Models/tokenizer-mistralai-7b-instruct\")\n",
    "model.save_pretrained(\"../Models/model-mistralai-7b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78e163bc-b2c7-4a53-9206-096851d436cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"\n",
    "Your task is to extract specific fields from the given input sentence and return the result strictly JSON format.\n",
    "\n",
    "Information:\n",
    "- Don't confuse theese two words \"current\" and \"last time\"\n",
    "\n",
    "\n",
    "Example:\n",
    "Input: \"What is the current temperature in the living room?\"\n",
    "Output: {\"time\": \"current\", \"room\": \"living room\", \"query\": \"temperature\"}\n",
    "\n",
    "Example:\n",
    "Input: \"What was the temperature in the bedroom yesterday?\"\n",
    "Output: {\"time\": \"yesterday\", \"room\": \"bedroom\", \"query\": \"temperature\"}\n",
    "\n",
    "Example:\n",
    "Input: \"What was the temperature in the bedroom last time?\"\n",
    "Output: {\"time\": \"last time\", \"room\": \"bedroom\", \"query\": \"temperature\"}\n",
    "\n",
    "Example:\n",
    "Input: \"What is the current humidity level in the sitting room?\"\n",
    "Output: {\"time\": \"current\", \"room\": \"sitting room\", \"query\": \"humidity\"}\n",
    "\n",
    "Example:\n",
    "Input: \"What is the current temperature in the bathroom?\"\n",
    "Output: {\"time\": \"current\", \"room\": \"bathroom\", \"query\": \"temperature\"}\n",
    "\n",
    "Now process the following sentence:\n",
    "Input: \"What was the temperature in the sitting room last time\"\n",
    "Output:\n",
    "\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e92cd941-a7e2-49d5-8276-4170f9bba18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"time\": \"last time\", \"room\": \"sitting room\", \"query\": \"temperature\"}\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n",
    "outputs = model.generate(inputs, max_new_tokens=150, temperature=0.0)\n",
    "\n",
    "result = tokenizer.decode(outputs[0][inputs.shape[-1]:], skip_special_tokens=True) # sadece modelin ürettiği yeni tokenleri decode et\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90efdfc5-b3c8-4d48-815a-bc75daa68e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_messages = [\n",
    "    {\n",
    "        \"role\" : \"user\",\n",
    "        \"content\" : \"\"\"\n",
    "Your goal is to answer the user's question using only the information provided to you.  \n",
    "- If the information is enough, give a clear and complete sentence as the answer, not just a number.  \n",
    "- If the information is not enough, say that you don’t have information about it.  \n",
    "\n",
    "Example:\n",
    "User Question: \"What was the temperature in the sitting room yesterday\"  \n",
    "Information: Date: 2025-08-15, Time: 9:00, Room: Sitting room, Sensor: Temperature Sensor, Sensor Value: 23.4, Sensor Unit: °C, Status: Low Temperature  \n",
    "Output: \"The recorded temperature in the sitting room was 23.4°C yesterday, which was considered low.\"\n",
    "\n",
    "Now process the following sentence:\n",
    "User Question: \"What is the current temperature in the living room\"\n",
    "Information: Date: 2025-08-16, Time: 15:00, Room: Living Room, Sensor: Temperature Sensor, Sensor Value: 25.4, Sensor Unit: °C, Status: Normal\n",
    "Output:\n",
    "\"\"\"\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adf4e66f-8f84-44d5-a19e-4454078f3c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The current temperature in the living room is 25.4°C, which is considered normal.\n"
     ]
    }
   ],
   "source": [
    "test_inputs = tokenizer.apply_chat_template(test_messages, return_tensors=\"pt\").to(model.device)\n",
    "test_outputs = model.generate(test_inputs, max_new_tokens=150, temperature=0)\n",
    "\n",
    "test_result = tokenizer.decode(test_outputs[0][test_inputs.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "print(test_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b33c3e-07ba-400e-abcf-0272cd9de8af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
